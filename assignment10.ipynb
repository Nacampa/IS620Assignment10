{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load assignment10.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# IS620 - Assignment 10\n",
      "# Program: assignment10.py\n",
      "# Student: Neil Acampa\n",
      "# Date:    10/31/16\n",
      "# Function:\n",
      "\n",
      "\n",
      "\n",
      "# 1. Choose a corpus of interest and perform a classification\n",
      "\n",
      "#    First choose the movie rating corpus\n",
      "#    Select top most frequent words (2000) without stop words\n",
      "#    Call document_features Train with Test with first 100 records\n",
      "\n",
      "#    Second Pass \n",
      "#    Select top most frequent words (2000) without stop words\n",
      "#    Call document_features Train on 90% and Test with 10%\n",
      "\n",
      "#    Third Pass \n",
      "#    Select top most frequent words (2000) include stop words\n",
      "#    Call document_features_all Train with Test with first 100 records\n",
      "\n",
      "#    Forth Pass \n",
      "#    Select top most frequent words (2000) include stop words\n",
      "#    Call document_features_all Train on 90% and Test with 10%\n",
      "\n",
      "\n",
      "#    Fith Pass\n",
      "#    Extract top 2000 most frequent words  \n",
      "#    Create a feature function that counts the frequency of word(i) in document(j) \n",
      "#    and assignes feature with frequency\n",
      "#    Use AFINN sentiment value (-5 to +5) and Harvard's Inquirybasic.xls with\n",
      "#    a word list designated Positive/Negative which is converted to a +1 and -1 respectively\n",
      "#    Each word feature is returned with a sentiment value\n",
      "\n",
      "\n",
      "#    Did not do this yet\n",
      "#    Evaluate the classifier: Show document's polarity verses overall document sentiment value\n",
      "#    see if they match\n",
      "\n",
      "#    Did not do this yet\n",
      "#    Evaluate the classifier (# of times Word(i) in doc(j) with word sentiment rating (k))\n",
      "#    Word(i) = \"Amazing\" occurs in Doc(j) = 5 times with Word Sentiment Rating(k) = 7 \n",
      "#    Return features {Contains Word(i): True  Frequency: 5 and Word Sentiment Rating:  35\n",
      "#    Currently shows something line Pos/Neg 11 to 1 - 11 times Positive\n",
      "#    Try to use Frequency and Rating\n",
      "\n",
      "#    For Fith read in AFINN-111.txt\n",
      "\n",
      "from __future__ import absolute_import \n",
      "from __future__ import division\n",
      "import re\n",
      "import os \n",
      "import math\n",
      "import decimal\n",
      "import numpy as np\n",
      "import scipy\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "from pandas import DataFrame\n",
      "import networkx as nx\n",
      "import random\n",
      "from urllib import urlopen\n",
      "import nltk\n",
      "nltk.download('gutenberg')\n",
      "from nltk import word_tokenize\n",
      "nltk.download('maxent_treebank_pos_tagger')\n",
      "nltk.download('punkt')\n",
      "nltk.download('movie_reviews')\n",
      "nltk.download('stopwords')\n",
      "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
      "from nltk.corpus import movie_reviews\n",
      "from nltk.corpus import stopwords\n",
      "stopwords = nltk.corpus.stopwords.words('english')\n",
      "\n",
      "\n",
      "linelst=[]\n",
      "lines  = \"\"\n",
      "allwords          = []   # Contains all words\n",
      "sentimentdict     = []   # Contains words from AFINN\n",
      "sentimentdictcnt  = []   # Contains a sentiment value (-5 to +5) for sentiment word\n",
      "sentiment         = []   # 2-D matrix sentiment word and value\n",
      "\n",
      " \n",
      "sentimentdoc      = []   # Total document sentiment value using AFINN\n",
      "sentimentdoccat   = []   # Document category\n",
      "dindx             = 0    # Document index\n",
      "\n",
      "\n",
      "masterdict        = []\n",
      "masterdictcnt     = []\n",
      "masterdictcntpos  = []\n",
      "masterdictcntneg  = []\n",
      "masterdictcat     = []\n",
      "# Table Elements\n",
      "\n",
      "fheadings      = [] \n",
      "\n",
      "fheadings.append(\"Words without stopwords                     \")\n",
      "fheadings.append(\"Words without stopwords                90/10\")\n",
      "fheadings.append(\"Words with stopwords                        \")\n",
      "fheadings.append(\"Words with stopwords                   90/10\")\n",
      "fheadings.append(\"Words without stopwords and Sentiment Value \")\n",
      "\n",
      "\n",
      "\n",
      "rejectchars = [',','.','?','<','>','!','\"','-','%','&','#','(',')','*',';'];\n",
      "rcnt = len(rejectchars);\n",
      "\n",
      "\n",
      "def remove_characters(word):\n",
      "  \"\"\"Replace special characters in the word\"\"\"\n",
      "  \n",
      "  for i in range(rcnt):\n",
      "    rchar = rejectchars[i]\n",
      "    if rchar in word:\n",
      "      word = word.replace(rchar,\"\")\n",
      "\n",
      "  return word\n",
      "\n",
      "\n",
      "def remove_symbols(word):\n",
      "  \"\"\"Replace symbols in the word\"\"\"\n",
      "  w = len(word)\n",
      "  word = (ord(c) for c in word) \n",
      "  word = map(lambda x:x if x<123 or x>255 else \" \", word)\n",
      "  newword=\"\"\n",
      "  for c in range(w):\n",
      "    if word[c] <> \" \":\n",
      "      newword += chr(word[c]);\n",
      "  \n",
      "  return newword\n",
      "\n",
      "\n",
      "def find_word(word, sentimentdict):\n",
      "  \"\"\"Find and return index of word in sentiment dictionary\"\"\"\n",
      "\n",
      "  masterlen = len(sentimentdict)\n",
      "  find=0\n",
      "  temp=\"x\"\n",
      "  try:\n",
      "   temp = sentimentdict.index(word);\n",
      "   return temp\n",
      "  except ValueError:\n",
      "   return temp\n",
      "\n",
      "\n",
      "\n",
      "def document_features(document):\n",
      "  docwords = set(document)\n",
      "  features = {}\n",
      "  for word in wordfeatures:\n",
      "    features['contains(%s)' % word] = (word in docwords)\n",
      " \n",
      "  return features\n",
      "\n",
      "\n",
      "def document_features_all(document):\n",
      "  docwords = set(document)\n",
      "  features = {}\n",
      "  for word in wordfeaturesall:\n",
      "    features['contains(%s)' % word] = (word in docwords)\n",
      " \n",
      "  return features\n",
      "\n",
      "\n",
      "def document_features_sentiment(document):\n",
      "  docwords = set(document)\n",
      "  features = {}\n",
      "  totalsentimentvalue = 0\n",
      "  sl = len(sentimentdict)\n",
      "  for word in wordfeaturesSent:\n",
      "    tword = word.encode('ascii')\n",
      "    findx = find_word(tword, sentimentdict)\n",
      "    sentimentval = 0\n",
      "    if (findx != \"x\"):\n",
      "      sentimentval = int(sentimentdictcnt[findx]) + 0\n",
      "      totalsentimentvalue = totalsentimentvalue + sentimentval\n",
      "    \n",
      "    \n",
      "    \n",
      "    features['contains(%s SV:%s)' % (word, sentimentval)] = (word in docwords)\n",
      "\n",
      " \n",
      "  return features\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "filepath=\"\"\n",
      "temp    =\"\"\n",
      "tokens  = \"\"\n",
      "valid   = 0\n",
      "p       = 1\n",
      "cwd = os.getcwd()\n",
      "corpus     = \"AFINN\"\n",
      "fullcorpus = \"AFINN-111.txt\"\n",
      "currfilepath = str(cwd) + \"\\AFINN-111.txt\"\n",
      "print currfilepath\n",
      "print (\"Enter the Full File Path including the File\")\n",
      "print (\"or Press return to use current File Path %s\") % (currfilepath)\n",
      "filepath = raw_input(\"Please enter the File Path now \")\n",
      "valid = 0\n",
      "if filepath == \"\":\n",
      "   filepath = currfilepath\n",
      "\n",
      " \n",
      "try:\n",
      "       f = open(filepath,\"r\")\n",
      "       try:\n",
      "         valid=1\n",
      "         x =0\n",
      "         j=0\n",
      "         for lines in f:\n",
      "           lines = lines.rstrip()\n",
      "           temp = lines.split(\"\\t\");\n",
      "           sentimentdict.append(temp[0])\n",
      "           sentimentdictcnt.append(temp[1])\n",
      "                            \n",
      "       finally:\n",
      "            f.close()\n",
      "         \n",
      "except IOError:\n",
      "       print (\"File not Found - Program aborting\")\n",
      "\n",
      "if not(valid):\n",
      "     exit()\n",
      "\n",
      "\n",
      "\n",
      "cwd = os.getcwd()\n",
      "corpus     = \"inquirebasic.txt\"\n",
      "fullcorpus = \"inquirebasic.txt\"\n",
      "currfilepath = str(cwd) + \"\\inquirebasic.csv\"\n",
      "print currfilepath\n",
      "print (\"Enter the Full File Path including the File\")\n",
      "print (\"or Press return to use current File Path %s\") % (currfilepath)\n",
      "filepath = raw_input(\"Please enter the File Path now \")\n",
      "valid = 0\n",
      "if filepath == \"\":\n",
      "   filepath = currfilepath\n",
      "\n",
      " \n",
      "try:\n",
      "       f = open(filepath,\"r\")\n",
      "       try:\n",
      "         valid=1\n",
      "         x =0\n",
      "         j=0\n",
      "         for lines in f:\n",
      "           lines = lines.rstrip()\n",
      "           temp = lines.split(\",\");\n",
      "           word = temp[0].lower()\n",
      "           sentimentdict.append(word)\n",
      "           if (temp[1] != \"\"):\n",
      "             # give positive words + 1\n",
      "             sentimentdictcnt.append(1)\n",
      "           else:\n",
      "             # give negative words -1 \n",
      "             if (temp[2] != \"\"):\n",
      "               sentimentdictcnt.append(-1)\n",
      "             else:\n",
      "               sentimentdictcnt.append(0)\n",
      "                            \n",
      "       finally:\n",
      "            f.close()\n",
      "         \n",
      "except IOError:\n",
      "       print (\"File not Found - Program aborting\")\n",
      "\n",
      "if not(valid):\n",
      "     exit()\n",
      " \n",
      "\n",
      "sentimentcnt = len(sentimentdict)\n",
      "for i in range(sentimentcnt):\n",
      "  sentiment.append([sentimentdict[i], sentimentdictcnt[i]])\n",
      "\n",
      "\n",
      " \n",
      "sl = len(sentimentdict)\n",
      "\n",
      "print\n",
      "print\n",
      "print(\"Getting Movie Review words\")\n",
      "results = []\n",
      "documents = [(list(movie_reviews.words(fileid)), category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)]\n",
      "random.shuffle(documents)\n",
      "dl = len(documents)\n",
      "\n",
      "print\n",
      "print(\"Getting top 2000 most frequent movie review words subtracting Stopwords\")\n",
      "# Top most frequent words in movie reviews without stopwords\n",
      "fd              = nltk.FreqDist(w.lower() for w in movie_reviews.words() if w.lower() not in stopwords)\n",
      "wordfeatures    = fd.keys()[:2000]\n",
      "\n",
      "print\n",
      "print(\"Getting top 2000 most frequent movie review words including Stopwords\")\n",
      "# Top most frequent words in movie reviews with stopwords\n",
      "fdall           = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
      "wordfeaturesall = fdall.keys()[:2000]\n",
      "\n",
      "\n",
      "print\n",
      "print(\"Getting top 200 most frequent words subtracting Stopwords for sentiment features\")\n",
      "# Top most frequent words in movie reviews without stopwords for sentiment value\n",
      "#fd              = nltk.FreqDist(w.lower() for w in movie_reviews.words() if w.lower() not in stopwords)\n",
      "wordfeaturesSent= fd.keys()[:500]\n",
      "\n",
      "\n",
      "\n",
      "# Features Positive/Negative with stop words removed\n",
      "print\n",
      "print(\"Processing Features without stopwords\")\n",
      "print(\"Training from Document 100 to 2000, Testing on the first 100\")\n",
      "print\n",
      "featuresets=[]\n",
      "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
      "train_set, test_set  = featuresets[100:], featuresets[:100]\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "accuracy   = nltk.classify.accuracy(classifier, test_set)\n",
      "results.append(accuracy)\n",
      "print(classifier.show_most_informative_features(10))\n",
      "\n",
      "\n",
      "# Features Positive/Negative with stop words removed\n",
      "print\n",
      "print(\"Processing Features without stop words\")\n",
      "print(\"Training on 90 percent, Testing on 10 percent\")\n",
      "print\n",
      "# Train on 90%, Test on 10%\n",
      "featuresets = []\n",
      "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
      "fcnt = len(featuresets)\n",
      "testlim  = int(fcnt*.10)\n",
      "trainlim = testlim +1\n",
      "train_set, test_set  = featuresets[trainlim:], featuresets[:testlim]\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "accuracy   = nltk.classify.accuracy(classifier, test_set)\n",
      "results.append(accuracy)\n",
      "print(classifier.show_most_informative_features(10))\n",
      "\n",
      "\n",
      "\n",
      "# Features Positive/Negative with stop words included\n",
      "print\n",
      "print(\"Processing Features with stop words included\")\n",
      "print(\"Training from Document 100 to 2000, Testing on the first 100\")\n",
      "print\n",
      "featuresets = []\n",
      "featuresets = [(document_features_all(d), c) for (d,c) in documents]\n",
      "train_set, test_set  = featuresets[100:], featuresets[:100]\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "accuracy   = nltk.classify.accuracy(classifier, test_set)\n",
      "results.append(accuracy)\n",
      "print(classifier.show_most_informative_features(10))\n",
      "\n",
      "print\n",
      "print(\"Processing Features with stop words included\")\n",
      "print(\"Training on 90 percent, Testing on 10 percent\")\n",
      "print\n",
      "featuresets = []\n",
      "featuresets = [(document_features_all(d), c) for (d,c) in documents]\n",
      "fcnt = len(featuresets)\n",
      "testlim  = int(fcnt*.10)\n",
      "trainlim = testlim +1\n",
      "train_set, test_set  = featuresets[trainlim:], featuresets[:testlim]\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "accuracy   = nltk.classify.accuracy(classifier, test_set)\n",
      "results.append(accuracy)\n",
      "print(classifier.show_most_informative_features(10))\n",
      "\n",
      "\n",
      "\n",
      "# Features Positive/Negative with stop words removed show word sentiment value\n",
      "print\n",
      "print(\"Processing Features without stopwords\")\n",
      "print(\"Training from Document 100 to 500, Testing on the first 100\")\n",
      "print(\"Display word with sentiment value\")\n",
      "print\n",
      "featuresets=[]\n",
      "featuresets = [(document_features_sentiment(d), c) for (d,c) in documents]\n",
      "train_set, test_set  = featuresets[100:], featuresets[:100]\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "accuracy   = nltk.classify.accuracy(classifier, test_set)\n",
      "results.append(accuracy)\n",
      "print(classifier.show_most_informative_features(10))\n",
      "\n",
      "\n",
      "\n",
      "print\n",
      "print\n",
      "l = len(results)\n",
      "print(l)\n",
      "print\n",
      "print (\"%s\\t%s\\t%s\") % (\"Feature\" , \"Feature Desc                              \", \"Accuracy\")\n",
      "indx = 0\n",
      "for i in range(l):\n",
      "  indx = indx + 1\n",
      "  print(\"%d\\t%s\\t%.4f\") % (indx, fheadings[i], results[i])\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run assignment10.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[nltk_data] Downloading package gutenberg to\n",
        "[nltk_data]     C:\\Users\\nacampa\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package gutenberg is already up-to-date!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
        "[nltk_data]     C:\\Users\\nacampa\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
        "[nltk_data]       date!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data] Downloading package punkt to\n",
        "[nltk_data]     C:\\Users\\nacampa\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package punkt is already up-to-date!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data] Downloading package movie_reviews to\n",
        "[nltk_data]     C:\\Users\\nacampa\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package movie_reviews is already up-to-date!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data] Downloading package stopwords to\n",
        "[nltk_data]     C:\\Users\\nacampa\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package stopwords is already up-to-date!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Anaconda2\\AFINN-111.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Enter the Full File Path including the File\n",
        "or Press return to use current File Path C:\\Anaconda2\\AFINN-111.txt\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Please enter the File Path now \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C:\\Anaconda2\\inquirebasic.csv\n",
        "Enter the Full File Path including the File\n",
        "or Press return to use current File Path C:\\Anaconda2\\inquirebasic.csv\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Please enter the File Path now \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Getting Movie Review words\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Getting top 2000 most frequent movie review words subtracting Stopwords\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Getting top 2000 most frequent movie review words including Stopwords\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Getting top 200 most frequent words subtracting Stopwords for sentiment features\n",
        "\n",
        "Processing Features without stopwords\n",
        "Training from Document 100 to 2000, Testing on the first 100\n",
        "\n",
        "Most Informative Features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "          contains(sans) = True              neg : pos    =      8.4 : 1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    contains(mediocrity) = True              neg : pos    =      7.7 : 1.0\n",
        "     contains(dismissed) = True              pos : neg    =      7.0 : 1.0\n",
        "         contains(wires) = True              neg : pos    =      6.4 : 1.0\n",
        "   contains(overwhelmed) = True              pos : neg    =      6.3 : 1.0\n",
        "     contains(uplifting) = True              pos : neg    =      5.8 : 1.0\n",
        "           contains(ugh) = True              neg : pos    =      5.8 : 1.0\n",
        "        contains(beware) = True              neg : pos    =      5.7 : 1.0\n",
        "       contains(topping) = True              pos : neg    =      5.6 : 1.0\n",
        "        contains(fabric) = True              pos : neg    =      5.6 : 1.0\n",
        "None\n",
        "\n",
        "Processing Features without stop words\n",
        "Training on 90 percent, Testing on 10 percent\n",
        "\n",
        "Most Informative Features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "          contains(sans) = True              neg : pos    =      7.6 : 1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "     contains(dismissed) = True              pos : neg    =      7.0 : 1.0\n",
        "    contains(mediocrity) = True              neg : pos    =      7.0 : 1.0\n",
        "   contains(overwhelmed) = True              pos : neg    =      6.4 : 1.0\n",
        "     contains(uplifting) = True              pos : neg    =      5.9 : 1.0\n",
        "       contains(topping) = True              pos : neg    =      5.7 : 1.0\n",
        "        contains(fabric) = True              pos : neg    =      5.7 : 1.0\n",
        "        contains(bounce) = True              neg : pos    =      5.6 : 1.0\n",
        "         contains(wires) = True              neg : pos    =      5.6 : 1.0\n",
        "        contains(beware) = True              neg : pos    =      5.6 : 1.0\n",
        "None\n",
        "\n",
        "Processing Features with stop words included\n",
        "Training from Document 100 to 2000, Testing on the first 100\n",
        "\n",
        "Most Informative Features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "          contains(sans) = True              neg : pos    =      8.4 : 1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    contains(mediocrity) = True              neg : pos    =      7.7 : 1.0\n",
        "     contains(dismissed) = True              pos : neg    =      7.0 : 1.0\n",
        "         contains(wires) = True              neg : pos    =      6.4 : 1.0\n",
        "   contains(overwhelmed) = True              pos : neg    =      6.3 : 1.0\n",
        "     contains(uplifting) = True              pos : neg    =      5.8 : 1.0\n",
        "           contains(ugh) = True              neg : pos    =      5.8 : 1.0\n",
        "        contains(beware) = True              neg : pos    =      5.7 : 1.0\n",
        "       contains(topping) = True              pos : neg    =      5.6 : 1.0\n",
        "        contains(fabric) = True              pos : neg    =      5.6 : 1.0\n",
        "None\n",
        "\n",
        "Processing Features with stop words included\n",
        "Training on 90 percent, Testing on 10 percent\n",
        "\n",
        "Most Informative Features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "          contains(sans) = True              neg : pos    =      7.6 : 1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "     contains(dismissed) = True              pos : neg    =      7.0 : 1.0\n",
        "    contains(mediocrity) = True              neg : pos    =      7.0 : 1.0\n",
        "   contains(overwhelmed) = True              pos : neg    =      6.4 : 1.0\n",
        "     contains(uplifting) = True              pos : neg    =      5.9 : 1.0\n",
        "       contains(topping) = True              pos : neg    =      5.7 : 1.0\n",
        "        contains(fabric) = True              pos : neg    =      5.7 : 1.0\n",
        "        contains(bounce) = True              neg : pos    =      5.6 : 1.0\n",
        "         contains(wires) = True              neg : pos    =      5.6 : 1.0\n",
        "        contains(beware) = True              neg : pos    =      5.6 : 1.0\n",
        "None\n",
        "\n",
        "Processing Features without stopwords\n",
        "Training from Document 100 to 500, Testing on the first 100\n",
        "Display word with sentiment value\n",
        "\n",
        "Most Informative Features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "contains(uplifting SV:0) = True              pos : neg    =      5.8 : 1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  contains(doubts SV:-1) = True              pos : neg    =      5.4 : 1.0\n",
        "contains(effortlessly SV:0) = True              pos : neg    =      5.3 : 1.0\n",
        "     contains(wang SV:0) = True              pos : neg    =      4.3 : 1.0\n",
        " contains(attorney SV:0) = True              pos : neg    =      4.3 : 1.0\n",
        "      contains(wcw SV:0) = True              neg : pos    =      3.7 : 1.0\n",
        "contains(wednesday SV:0) = True              pos : neg    =      3.7 : 1.0\n",
        "      contains(hal SV:0) = True              neg : pos    =      3.4 : 1.0\n",
        "contains(nonsensical SV:0) = True              neg : pos    =      3.4 : 1.0\n",
        "contains(portrayed SV:0) = True              pos : neg    =      3.4 : 1.0\n",
        "None\n",
        "\n",
        "\n",
        "5\n",
        "\n",
        "Feature\tFeature Desc                              \tAccuracy\n",
        "1\tWords without stopwords                     \t0.6800\n",
        "2\tWords without stopwords                90/10\t0.6700\n",
        "3\tWords with stopwords                        \t0.6600\n",
        "4\tWords with stopwords                   90/10\t0.6500\n",
        "5\tWords without stopwords and Sentiment Value \t0.6300\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}